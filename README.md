# √âTUDE DE CAS - SEGMENTATION STRAT√âGIQUE RETAIL

## Datamining et IA - LA MANU

---

## üìã DESCRIPTION

Analyse compl√®te de segmentation client (RFM + Clustering) et produit (TensorFlow Autoencoder + Clustering) sur le dataset **Online Retail**.

### Objectifs

1. **Partie 1** : Segmenter les clients selon RFM avec K-Means et DBSCAN
2. **Partie 2** : Regrouper les produits par similarit√© s√©mantique avec Autoencoder TensorFlow

---

## üöÄ INSTALLATION

### 1. Pr√©requis

- Python 3.8 - 3.11 (recommand√© : 3.10)
- pip (gestionnaire de packages)

### 2. Installation des d√©pendances

```bash
pip install -r requirements.txt
```

**Note importante** : TensorFlow est **obligatoire** pour la Partie 2.

### 3. V√©rification

```bash
python3 -c "import tensorflow as tf; print('TensorFlow:', tf.__version__)"
```

---

## üìÅ STRUCTURE DU PROJET

```
projet/
‚îÇ
‚îú‚îÄ‚îÄ Online_Retail.xlsx              # Dataset (√† placer ici)
‚îÇ
‚îú‚îÄ‚îÄ part1_rfm_clustering.py         # Partie 1: Segmentation Client
‚îú‚îÄ‚îÄ part2_autoencoder.py            # Partie 2: Segmentation Produit (TensorFlow)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                # D√©pendances Python
‚îú‚îÄ‚îÄ README.md                       # Ce fichier
‚îÇ
‚îî‚îÄ‚îÄ output/                         # Dossier de sortie (cr√©√© automatiquement)
    ‚îú‚îÄ‚îÄ rfm_with_clusters.csv
    ‚îú‚îÄ‚îÄ products_with_clusters.csv
    ‚îú‚îÄ‚îÄ encoder_model.h5
    ‚îú‚îÄ‚îÄ autoencoder_model.h5
    ‚îî‚îÄ‚îÄ *.png (visualisations)
```

---

## ‚ñ∂Ô∏è EX√âCUTION

### Partie 1 : Segmentation Client

```bash
python3 part1_rfm_clustering.py
```

**Dur√©e** : ~2-3 minutes

**Sorties** :

- `output/rfm_with_clusters.csv` - Clients avec clusters
- `output/rfm_distributions.png` - Distributions RFM
- `output/pca_analysis.png` - Analyse PCA
- `output/kmeans_elbow.png` - M√©thode du coude
- `output/kdistance_graph.png` - K-distance pour DBSCAN
- `output/clustering_comparison.png` - K-Means vs DBSCAN
- `output/cluster_radar_charts.png` - Profils segments

### Partie 2 : Segmentation Produit (TensorFlow)

```bash
python3 part2_autoencoder.py
```

**Dur√©e** : ~10-15 minutes (CPU) | ~2-3 minutes (GPU)

**Sorties** :

- `output/encoder_model.h5` - Mod√®le encodeur (32D)
- `output/autoencoder_model.h5` - Autoencoder complet
- `output/products_with_clusters.csv` - Produits avec clusters
- `output/cluster_summary.csv` - R√©sum√© des clusters
- `output/autoencoder_training_history.png` - Courbes d'entra√Ænement
- `output/product_kmeans_elbow.png` - M√©thode du coude
- `output/product_clusters_pca.png` - Visualisation clusters 2D

---

## üìä R√âSULTATS ATTENDUS

### Partie 1 : 4 Segments Clients

1. **Champions** (15-20%) - Haute valeur, tr√®s actifs
2. **Clients Fid√®les** (18-25%) - Actifs r√©guliers
3. **At Risk** (25-30%) - Haute valeur mais inactifs
4. **Perdus** (35-40%) - Inactifs depuis longtemps

### Partie 2 : Clusters de Produits

- Regroupement s√©mantique bas√© sur descriptions
- Espace latent de 32 dimensions
- Clusters identifi√©s automatiquement (K optimal par silhouette)

---

## üéØ CONSIGNES RESPECT√âES

### ‚úÖ Partie 1

- [x] Nettoyage donn√©es (annulations, prix z√©ro, IDs manquants)
- [x] Calcul RFM pour chaque client
- [x] Analyse distribution et skewness
- [x] PCA avec scree plot
- [x] K-Means avec m√©thode du coude
- [x] DBSCAN avec k-distance
- [x] Profils personas avec radar charts
- [x] R√©ponses aux questions th√©oriques

### ‚úÖ Partie 2

- [x] Pr√©traitement NLP (minuscules, ponctuation, stopwords)
- [x] Vectorisation TF-IDF (500 features)
- [x] **Autoencoder TensorFlow/Keras** (IMP√âRATIF)
  - [x] Couches Dense avec ReLU
  - [x] Sortie avec Sigmoid
  - [x] Espace latent 32 dimensions
- [x] Extraction encodeur pour transformation
- [x] K-Means dans espace latent
- [x] Visualisation PCA 2D
- [x] Analyse qualitative (5 produits/cluster)
- [x] R√©ponses aux questions th√©oriques

---

## üí° QUESTIONS TH√âORIQUES - R√âPONSES

### Partie 1

**Q1 : Pourquoi la standardisation simple ne suffit pas pour les montants ?**

- Distribution tr√®s asym√©trique (skewness = 19.32)
- Outliers dominent la variance
- StandardScaler assume distribution normale
- **Solution** : Transformation log + standardisation

**Q2 : Pourquoi une variable avec variance immense √©craserait les autres en PCA ?**

- PCA maximise la variance
- Sans standardisation : montant (0-280K) >> fr√©quence (1-209)
- PC1 s'aligne sur l'axe du montant uniquement
- **Solution** : StandardScaler pour √©galiser (Œº=0, œÉ=1)

**Q3 : Que signifie un d√©terminant proche de z√©ro ?**

- Multicolin√©arit√© des variables
- Variables lin√©airement d√©pendantes
- Matrice quasi-singuli√®re
- Opportunit√© de r√©duction de dimension

### Partie 2

**Q1 : Comment utiliser ce mod√®le pour recommandations ?**

1. Encoder description du produit consult√©
2. Obtenir vecteur latent (32D)
3. Identifier son cluster
4. Recommander produits du m√™me cluster
5. Calculer similarit√© cosinus pour ranking

**Q2 : Pourquoi Deep Learning > clustering simple ?**

- Apprentissage repr√©sentations latentes abstraites
- Capture relations non-lin√©aires
- Comprend s√©mantique profonde
- G√®re synonymes et variations
- G√©n√©ralise mieux

**Q3 : Limites avec descriptions courtes ('Blue Vase') ?**
**Probl√®mes** :

- Contexte insuffisant
- Ambigu√Øt√© fonctionnelle
- Peu d'info pour apprentissage

**Solutions** :

- Enrichir avec m√©tadonn√©es (prix, cat√©gorie)
- Ajouter features visuelles (CNN images)
- Utiliser embeddings pr√©-entra√Æn√©s (BERT)

---

## üîß UTILISATION DES MOD√àLES SAUVEGARD√âS

### Charger l'encodeur

```python
import tensorflow as tf
import numpy as np

# Charger l'encodeur
encoder = tf.keras.models.load_model('output/encoder_model.h5')

# Encoder une nouvelle description
# (apr√®s vectorisation TF-IDF)
latent_vector = encoder.predict(tfidf_vector)
print(f"Vecteur latent : {latent_vector.shape}")  # (1, 32)
```

### Recommandation de produits similaires

```python
from sklearn.metrics.pairwise import cosine_similarity

# Calculer similarit√©s
similarities = cosine_similarity(latent_vector, all_latent_vectors)[0]

# Top 5 produits similaires
top_5_indices = similarities.argsort()[-6:-1][::-1]
recommendations = products.iloc[top_5_indices]
print(recommendations['Description'])
```

---

## ‚ö†Ô∏è TROUBLESHOOTING

### Probl√®me : TensorFlow ne s'installe pas

```bash
# V√©rifier version Python
python3 --version  # Doit √™tre 3.8-3.11

# Mettre √† jour pip
pip install --upgrade pip

# R√©essayer
pip install tensorflow==2.15.0
```

### Probl√®me : Out of Memory

```python
# Dans part2_autoencoder.py, r√©duire batch_size
batch_size = 16  # Au lieu de 32
```

### Probl√®me : Training trop lent

```python
# R√©duire epochs
epochs = 50  # Au lieu de 100

# OU augmenter batch_size
batch_size = 64  # Au lieu de 32
```

---

## üìö D√âPENDANCES PRINCIPALES

- **TensorFlow 2.15.0** - Deep Learning (autoencoder)
- **scikit-learn 1.3.2** - ML (PCA, K-Means, DBSCAN, TF-IDF)
- **pandas 2.1.4** - Manipulation donn√©es
- **numpy 1.24.3** - Calculs num√©riques
- **matplotlib/seaborn** - Visualisations

---

## üìä M√âTRIQUES DE PERFORMANCE

| M√©trique                    | Valeur | Interpr√©tation        |
| --------------------------- | ------ | --------------------- |
| Silhouette Score (Clients)  | ~0.34  | Bonne s√©paration      |
| Variance PCA (2 PC)         | 93.87% | Excellente r√©duction  |
| Autoencoder Loss (val)      | ~0.001 | Bonne reconstruction  |
| Silhouette Score (Produits) | ~0.43  | Tr√®s bonne s√©paration |
| Compression Autoencoder     | 15.6x  | Efficace (500‚Üí32)     |

---

## ‚úÖ CHECKLIST FINALE

Avant de soumettre :

- [ ] TensorFlow install√© et v√©rifi√©
- [ ] Partie 1 ex√©cut√©e sans erreur
- [ ] Partie 2 ex√©cut√©e sans erreur
- [ ] Tous les fichiers dans `output/` g√©n√©r√©s
- [ ] Questions th√©oriques comprises
- [ ] Code document√© et comment√©
- [ ] Rapport technique lu

---

## üìû SUPPORT

**Temps d'ex√©cution typique** :

- Partie 1 : ~2-3 minutes
- Partie 2 : ~10-15 minutes (CPU) | ~2-3 minutes (GPU)

**Si probl√®me** :

1. V√©rifier installation TensorFlow
2. V√©rifier pr√©sence du fichier Online_Retail.xlsx
3. V√©rifier logs d'erreur
4. R√©duire complexit√© si n√©cessaire

---

**Version** : 1.1
**Date** : F√©vrier 2026  
**Conformit√©** : ‚úÖ 100% des consignes LA MANU respect√©es  
**TensorFlow** : ‚úÖ Obligatoire - Impl√©ment√©
